{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function Libraries"
      ],
      "metadata": {
        "id": "18-m9m8xP28-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install Libraries\n",
        "!pip install fastapi python-multipart uvicorn\n",
        "!pip install -U kaleido"
      ],
      "metadata": {
        "id": "bKOkasb0P4hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Libraries\n",
        "# These are used to export an image\n",
        "import plotly.io\n",
        "from plotly.io import to_image\n",
        "# These will be used to load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# These will be used to create images\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "# This is the library for candlestick charts\n",
        "import plotly.graph_objects as go\n",
        "# Misc. Libraries\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "kEBs9lirQCv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling Function"
      ],
      "metadata": {
        "id": "F6JkiTJe7Oe-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUoVMcA57LB9"
      },
      "outputs": [],
      "source": [
        "def labeling(df):\n",
        "    df_new_col = pd.DataFrame()\n",
        "    for date in df['Date'].unique():\n",
        "        # Filter so that it does not calculate price change from 1 day to the next\n",
        "        single_day = df[df['Date'] == date].copy()\n",
        "        single_day.dropna(subset=['Close_2m'], inplace=True)\n",
        "        # Calculate the 6 minute change in price\n",
        "        single_day['SixMinChange'] = single_day['Close_2m'].shift(-3) - single_day['Close_2m']\n",
        "        # concatenate data into a new data frame\n",
        "        df_new_col = pd.concat([df_new_col, single_day], ignore_index=True)\n",
        "\n",
        "    df = df_new_col.copy()\n",
        "    # Labeling conditions: top 33% are considered bull, bottome 33 percent are considered bear\n",
        "    percentiles = [.67, .33]\n",
        "\n",
        "    # This calculates the price to seperate labels on\n",
        "    bull_condition = df['SixMinChange'].quantile(percentiles[0])\n",
        "    bear_condition = df['SixMinChange'].quantile(percentiles[1])\n",
        "\n",
        "    # Create 'Label' column based on conditions\n",
        "    df['SixMinLabel'] = 'Neutral'\n",
        "    df.loc[df['SixMinChange'] > bull_condition, 'SixMinLabel'] = 'Bullish'\n",
        "    df.loc[df['SixMinChange'] < bear_condition, 'SixMinLabel'] = 'Bearish'\n",
        "\n",
        "    # Drop NaNs after calculations\n",
        "    df.dropna(subset=['SixMinChange'], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum Indicator Creation"
      ],
      "metadata": {
        "id": "snSKZ1FM900i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_columns(df):\n",
        "  df_new_cols = pd.DataFrame()\n",
        "  for date in df['Date'].unique():\n",
        "    # Find the Moving Averages and Hourly Change\n",
        "    single_day = df[df['Date'] == date].copy()\n",
        "    single_day['TenMinMovingAvg'] = single_day['Close_2m'].rolling(window=5).mean()\n",
        "    single_day['TwentyMinMovingAvg'] = single_day['Close_2m'].rolling(window=10).mean()\n",
        "    single_day['ThirtyMinMovingAvg'] = single_day['Close_2m'].rolling(window=15).mean()\n",
        "    single_day['HourChange'] = single_day['Close_2m'].shift(30) - single_day['Close_2m']\n",
        "    df_new_cols = pd.concat([df_new_cols, single_day], ignore_index=True)\n",
        "  df = df_new_cols\n",
        "  ## Create Labels for Dummy Variables\n",
        "  # 10 Minute MA\n",
        "  df['TenMinMALabel'] = 'Neutral'\n",
        "  df.loc[100*df['Close_2m'] > 100*df['TenMinMovingAvg'], 'TenMinMALabel'] = 'Above'\n",
        "  df.loc[100*df['Close_2m'] < 100*df['TenMinMovingAvg'], 'TenMinMALabel'] = 'Below'\n",
        "\n",
        "  # 20 Minute MA\n",
        "  df['TwentyMinMALabel'] = 'Neutral'\n",
        "  df.loc[100*df['Close_2m'] > 100*df['TwentyMinMovingAvg'], 'TwentyMinMALabel'] = 'Above'\n",
        "  df.loc[100*df['Close_2m'] < 100*df['TwentyMinMovingAvg'], 'TwentyMinMALabel'] = 'Below'\n",
        "\n",
        "  # Thirty Minute MA\n",
        "  df['ThirtyMinMALabel'] = 'Neutral'\n",
        "  df.loc[100*df['Close_2m'] > 100*df['ThirtyMinMovingAvg'], 'ThirtyMinMALabel'] = 'Above'\n",
        "  df.loc[100*df['Close_2m'] < 100*df['ThirtyMinMovingAvg'], 'ThirtyMinMALabel'] = 'Below'\n",
        "\n",
        "  # Ten Cross Twently Label\n",
        "  df['TenCrossTwenty'] = 'Neutral'\n",
        "  df.loc[100*df['TenMinMovingAvg'] > 100*df['TwentyMinMovingAvg'], 'TenCrossTwenty'] = 'Above'\n",
        "  df.loc[100*df['TenMinMovingAvg'] < 100*df['TwentyMinMovingAvg'], 'TenCrossTwenty'] = 'Below'\n",
        "\n",
        "  # Ten Cross Twently Label\n",
        "  df['TenCrossThirty'] = 'Neutral'\n",
        "  df.loc[100*df['TenMinMovingAvg'] > 100*df['ThirtyMinMovingAvg'], 'TenCrossThirty'] = 'Above'\n",
        "  df.loc[100*df['TenMinMovingAvg'] < 100*df['ThirtyMinMovingAvg'], 'TenCrossThirty'] = 'Below'\n",
        "\n",
        "  # Hourly Change\n",
        "  df['HourChangeLabel'] = 'Neutral'\n",
        "  df.loc[10*df['HourChange'] > 0, 'HourChangeLabel'] = 'Above'\n",
        "  df.loc[10*df['HourChange'] < 0, 'HourChangeLabel'] = 'Below'\n",
        "\n",
        "  # Drop nulls\n",
        "  df.dropna(subset=['HourChange'], inplace = True)\n",
        "  df = df.reset_index()\n",
        "  return df"
      ],
      "metadata": {
        "id": "UEFRRns79cSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "gBTqMpH1-VF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intraday_cleaning(data):\n",
        "    # Change to datetime data type, normalize to utc time zone\n",
        "    data['Datetime'] = pd.to_datetime(data['Datetime'], utc=True)\n",
        "    # Create a Date column\n",
        "    data['Date'] = data['Datetime'].dt.date\n",
        "    # Create a time column\n",
        "    data['Time'] = data['Datetime'].dt.time\n",
        "    # Adj Close is not used and Datetime become repetitive\n",
        "    data.drop(['Datetime', 'Adj Close'], inplace=True, axis=1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "CdHaTHv9-9xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Candlestick Creation"
      ],
      "metadata": {
        "id": "vFxjIXaw_SjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def candle_sticks(data, image_folder_base, ticker, candles, step):\n",
        "    # Assuming 'Date' and 'Label' columns exist in the dataframe\n",
        "    unique_dates = data['Date'].unique()\n",
        "\n",
        "    for date in unique_dates:\n",
        "        day_df = data.loc[data['Date'] == date]\n",
        "\n",
        "        # Select 10-period sequences\n",
        "        start_candle = 0\n",
        "        end_candle = candles\n",
        "\n",
        "        while end_candle <= len(day_df):\n",
        "            # Filter the dataframe for the sequence\n",
        "            sequence = day_df.iloc[start_candle:end_candle]\n",
        "\n",
        "            # Create candlestick chart\n",
        "            fig = go.Figure(data=[go.Candlestick(x=sequence['Time'],\n",
        "                                                 open=sequence['Open_2m'],\n",
        "                                                 high=sequence['High_2m'],\n",
        "                                                 low=sequence['Low_2m'],\n",
        "                                                 close=sequence['Close_2m'])])\n",
        "\n",
        "            # Get label for the last candlestick in the sequence\n",
        "            label = sequence['SixMinLabel'].iloc[-1]\n",
        "            output_folder = f'{image_folder_base}/{label}'\n",
        "\n",
        "            # Write to a JPEG file\n",
        "            image_file = f\"{output_folder}/candles_{date}_{ticker}_{start_candle}_{end_candle}_label_{label}.jpeg\"\n",
        "            fig.write_image(image_file)\n",
        "\n",
        "            # Move to the next sequence\n",
        "            start_candle += step\n",
        "            end_candle += step"
      ],
      "metadata": {
        "id": "dKwAY47P_W1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Candlestick Image Dataset"
      ],
      "metadata": {
        "id": "AFSx6hDNR7IV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the function to get load images into a dataset using the folders in google drive"
      ],
      "metadata": {
        "id": "vkvL5ahaSA9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CandlestickDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        label_encoder = LabelEncoder()  # Initialize label encoder\n",
        "\n",
        "        for label in os.listdir(self.root_dir):\n",
        "            label_dir = os.path.join(self.root_dir, label)\n",
        "            if os.path.isdir(label_dir):\n",
        "                for image_file in os.listdir(label_dir):\n",
        "                    self.image_paths.append(os.path.join(label_dir, image_file))\n",
        "                    self.labels.append(label)  # Store original labels\n",
        "\n",
        "        # Encode labels\n",
        "        self.labels = label_encoder.fit_transform(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Open image and convert to RGB (if grayscale)\n",
        "        image = Image.open(img_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "xhRSx1m2R6qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "NUVHpVPNP5fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model Libraries\n",
        "# PyTorch and dependencies\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "# Model\n",
        "import torchvision.models as models\n",
        "from torchvision.models import densenet121\n",
        "# For image/data loading and export\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "z_gvHDoFP7lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample of the work flow to get images from a folder, into the model, and output predictions"
      ],
      "metadata": {
        "id": "rykkz8zQQ_eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This only works if working in colab, connects google drive where everything is stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "## This is the data preprocessing function\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_batch_size = 256\n",
        "\n",
        "## This is the Nasdaq filepath for google drive\n",
        "nq_path =  '/content/drive/MyDrive/Algo_Trader/FuturesTrading/CNN Model/Images/TestImages/NQF'\n",
        "## This transforms the images into a dataset using the candelstickdataset function and preprocessing function\n",
        "nq_set = CandlestickDataset(root_dir=nq_path, transform=test_transform)\n",
        "## This loads the dataset into a dataloader for more efficient training/testing\n",
        "nq_loader = DataLoader(nq_set, batch_size=test_batch_size, pin_memory=True)"
      ],
      "metadata": {
        "id": "slVCSijhQ9HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting up the model\n",
        "## This is the file path to the model weights\n",
        "model_weights = '/content/drive/MyDrive/Algo_Trader/FuturesTrading/CNN Model/Data/Weights/DenseNet-121_epoch_19.pt'\n",
        "\n",
        "## This loads the model architecture\n",
        "model = densenet121(pretrained=False)\n",
        "\n",
        "## The original architecture has 1000 neurons in the output layer, we only need 3 so this adjusts that\n",
        "num_ftrs = model.classifier.in_features\n",
        "model.classifier = nn.Linear(num_ftrs, 3)\n",
        "\n",
        "## This loads the weights into the model\n",
        "loaded_weights = torch.load(model_weights)\n",
        "model.load_state_dict(loaded_weights)\n",
        "\n",
        "## This moves the model to the GPU\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "o5aChc64TPHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions = []  # Store predictions for individual images\n",
        "output_values = []  # Store output values for individual images\n",
        "true_labels = [] # Store the true labels for individual images\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "## This is the inference/prediction code\n",
        "with torch.no_grad():\n",
        "    # Takes the images and labels in the loader and stores them then moves them to the GPU\n",
        "    for images, labels in nq_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Predicts values for each class\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculates the probabilities for each class using Softmax\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "        # Finds the highest probability for each image\n",
        "        _, predicted = probabilities.max(1)\n",
        "\n",
        "        # Collect predictions and output values\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        output_values.extend(probabilities.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Flatten the output values list\n",
        "output_values_flat = np.array(output_values).reshape(-1, 3)  # Assuming 3 classes\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {\n",
        "    'Predicted_Class': all_predictions,\n",
        "    'Actual_Class':true_labels,\n",
        "    'Class_0_Score': output_values_flat[:, 0],\n",
        "    'Class_1_Score': output_values_flat[:, 1],\n",
        "    'Class_2_Score': output_values_flat[:, 2]\n",
        "}\n",
        "\n",
        "# Create a DataFrame with above outputs generated by the model\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Export to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Algo_Trader/FuturesTrading/CNN Model/Data/Results/model_results.csv')"
      ],
      "metadata": {
        "id": "FOA6Jk1nT6_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}